/**
 * Voice Bot - Voice-only AI Guide
 * Reads screen content and provides voice guidance
 */

import { useState, useEffect, useRef } from 'react'
import { useLocation } from 'react-router-dom'
import { motion, AnimatePresence } from 'framer-motion'
import { speakWithElevenLabs } from '../utils/elevenLabs'
import mobileSpeech from '../utils/mobileSpeech'
import screenAnalyzer from '../utils/screenAnalyzer'
import './VoiceBot.css'

const VoiceBot = ({ 
  mode = 'general', // 'onboarding', 'test', 'game', 'results'
  testType = null,
  screenContent = null // Content to read from screen
}) => {
  const location = useLocation()
  const [isActive, setIsActive] = useState(false)
  const [isSpeaking, setIsSpeaking] = useState(false)
  const [isListening, setIsListening] = useState(false)
  const [isQuestionMode, setIsQuestionMode] = useState(false)
  const [currentQuestion, setCurrentQuestion] = useState(null)
  const [currentAnswer, setCurrentAnswer] = useState(null)
  const [questionIndex, setQuestionIndex] = useState(0)
  const recognitionRef = useRef(null)
  const speechQueueRef = useRef([])
  const isProcessingRef = useRef(false)
  const screenAnalysisRef = useRef(null)

  // Initialize voice recognition
  useEffect(() => {
    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition
      recognitionRef.current = new SpeechRecognition()
      recognitionRef.current.continuous = true
      recognitionRef.current.interimResults = false
      recognitionRef.current.lang = 'en-US'
      
      recognitionRef.current.onresult = (event) => {
        const transcript = event.results[event.results.length - 1][0].transcript
        if (isQuestionMode) {
          handleAnswer(transcript)
        } else {
          handleVoiceCommand(transcript)
        }
      }
      
      recognitionRef.current.onerror = (event) => {
        console.error('Speech recognition error:', event.error)
        setIsListening(false)
      }
    }

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop()
      }
    }
  }, [])

  // Auto-activate and welcome based on route
  useEffect(() => {
    const path = location.pathname
    
    // Auto-activate on testing pages
    if (path.includes('vision-tests') || path.includes('eye-scan') || path.includes('mini-games')) {
      setIsActive(true)
      setTimeout(() => {
        provideWelcomeGuidance()
      }, 1000)
    } else if (path === '/onboarding') {
      setIsActive(true)
      setTimeout(() => {
        speak('Welcome to Quick Optics AI! I\'ll guide you through the setup process. Swipe through the slides to learn more.')
      }, 500)
    } else if (path === '/results') {
      setIsActive(true)
      setTimeout(() => {
        speak('Here are your vision test results. I can help explain what they mean.')
      }, 1000)
    }
  }, [location.pathname, mode, testType])

  // Silently analyze screen content when it changes
  useEffect(() => {
    if (screenContent && isActive) {
      // Analyze silently without speaking
      const analysis = screenAnalyzer.analyze(screenContent)
      screenAnalysisRef.current = analysis
      
      // Only speak if guidance is needed
      const needsCommunication = screenAnalyzer.getCommunicationNeeds(screenContent)
      if (needsCommunication !== 'none') {
        provideContextualGuidance(screenContent, needsCommunication)
      }
    }
  }, [screenContent, isActive])

  const speak = async (text) => {
    if (!text || isProcessingRef.current) return Promise.resolve()
    
    isProcessingRef.current = true
    setIsSpeaking(true)

    return new Promise((resolve) => {
      const finishSpeaking = () => {
        setIsSpeaking(false)
        isProcessingRef.current = false
        processQueue()
        resolve()
      }

      try {
        // Try ElevenLabs first, then fallback to enhanced mobile speech
        speakWithElevenLabs(text)
          .then(() => {
            finishSpeaking()
          })
          .catch(async (error) => {
            console.error('ElevenLabs speech error:', error)
            // Enhanced mobile-optimized fallback
            try {
              await mobileSpeech.speak(text, {
                rate: 0.9,
                pitch: 1.2,
                preferFemale: true
              })
              finishSpeaking()
            } catch (mobileError) {
              console.error('Mobile speech error:', mobileError)
              // Final fallback to basic browser TTS
              if ('speechSynthesis' in window) {
                const utterance = new SpeechSynthesisUtterance(text)
                utterance.rate = 0.9
                utterance.pitch = 1.2
                utterance.onend = finishSpeaking
                utterance.onerror = finishSpeaking
                window.speechSynthesis.speak(utterance)
              } else {
                finishSpeaking()
              }
            }
          })
      } catch (error) {
        console.error('Speech setup error:', error)
        finishSpeaking()
      }
    })
  }

  const processQueue = () => {
    if (speechQueueRef.current.length > 0 && !isProcessingRef.current) {
      const nextText = speechQueueRef.current.shift()
      speak(nextText)
    }
  }

  const queueSpeech = (text) => {
    speechQueueRef.current.push(text)
    if (!isProcessingRef.current) {
      processQueue()
    }
  }

  const provideContextualGuidance = (content, needs) => {
    if (needs === 'welcome') {
      if (mode === 'onboarding') {
        speak('Welcome to Quick Optics AI! Swipe through to learn more about our vision testing.')
      }
    } else if (needs === 'guidance') {
      if (content.description) {
        speak(content.description)
      }
    } else if (needs === 'instruction') {
      if (mode === 'test' && testType) {
        startTestQuestions()
      }
    }
  }

  const startTestQuestions = () => {
    setIsQuestionMode(true)
    setQuestionIndex(0)
    askNextQuestion(0)
  }

  const askNextQuestion = (index) => {
    const questions = getTestQuestions(testType)
    if (index < questions.length) {
      const question = questions[index]
      setCurrentQuestion(question)
      setQuestionIndex(index)
      speak(question.question).then(() => {
        setIsListening(true)
        if (recognitionRef.current) {
          recognitionRef.current.start()
        }
      })
    } else {
      // All questions answered
      setIsQuestionMode(false)
      setCurrentQuestion(null)
      speak('Thank you for answering. Let\'s continue with the test.')
    }
  }

  const getTestQuestions = (type) => {
    const questionSets = {
      'eye-scan': [
        { question: 'Are you in a well-lit room?', key: 'lighting' },
        { question: 'Can you see the alignment frame clearly?', key: 'visibility' },
        { question: 'Are you comfortable and ready to begin?', key: 'ready' }
      ],
      'myopia': [
        { question: 'Can you read the letters on the screen?', key: 'readability' },
        { question: 'Are the letters clear or blurry?', key: 'clarity' }
      ],
      'hyperopia': [
        { question: 'Which text appears clearer, near or far?', key: 'focus' }
      ],
      'astigmatism': [
        { question: 'Do all the lines appear equally clear?', key: 'uniformity' }
      ],
      'color': [
        { question: 'Can you see the numbers in the colored circles?', key: 'colorVision' }
      ]
    }
    
    return questionSets[type] || []
  }

  const provideWelcomeGuidance = () => {
    let welcomeText = ''
    
    if (mode === 'test') {
      welcomeText = getTestWelcome(testType)
    } else if (mode === 'game') {
      welcomeText = 'Welcome to Vision Trainer! I\'ll guide you through each mini-game. Listen carefully to my instructions.'
    } else {
      welcomeText = 'Welcome! I\'m your voice guide. I\'ll help you through the vision testing process. Just listen and follow my instructions.'
    }
    
    speak(welcomeText)
  }

  const getTestWelcome = (type) => {
    const welcomes = {
      'eye-scan': 'Welcome to the Eye Scan! Please look straight at the camera and hold still. I will guide you through the scanning process.',
      'myopia': 'Starting the Myopia Test. This checks for nearsightedness. Read the letters as they appear and tell me what you see.',
      'hyperopia': 'Next is the Hyperopia Test. This measures farsightedness. Focus on the text and tell me which one is clearer.',
      'astigmatism': 'This is the Astigmatism Test. Look at the line patterns and identify which one appears clearest.',
      'color': 'Time for the Color Blindness Test. Look at the colored dots and tell me what number you see.',
      'contrast': 'Now the Contrast Sensitivity Test. Choose the shape with the best contrast.',
      'dry-eye': 'Finally, the Dry Eye Risk Test. Please answer the questions honestly.'
    }
    return welcomes[type] || 'Welcome to the vision test. I\'ll guide you through each step.'
  }

  const handleAnswer = (answer) => {
    setCurrentAnswer(answer)
    setIsListening(false)
    
    if (recognitionRef.current) {
      recognitionRef.current.stop()
    }

    // Confirm the answer
    const confirmations = [
      'Got it, thank you.',
      'Understood.',
      'Perfect, I\'ve noted that.',
      'Thank you for that information.'
    ]
    const confirmation = confirmations[Math.floor(Math.random() * confirmations.length)]
    
    speak(confirmation).then(() => {
      // Move to next question
      const nextIndex = questionIndex + 1
      setTimeout(() => {
        askNextQuestion(nextIndex)
      }, 1000)
    })
  }

  const handleVoiceCommand = (command) => {
    const lowerCommand = command.toLowerCase()
    
    // More conversational responses like ChatGPT
    if (lowerCommand.includes('hello') || lowerCommand.includes('hi')) {
      speak('Hello! I\'m Dr. AI, your virtual eye care assistant. I\'m here to guide you through your vision examination. How are you feeling about the test today?')
    } else if (lowerCommand.includes('nervous') || lowerCommand.includes('scared') || lowerCommand.includes('worried')) {
      speak('I completely understand those feelings. Eye exams can seem intimidating, but I want to assure you this is completely painless and safe. Think of it like taking a smart selfie that can tell us about your eye health. I\'ll be with you every step of the way.')
    } else if (lowerCommand.includes('how long') || lowerCommand.includes('duration') || lowerCommand.includes('time')) {
      speak('The entire screening takes just 3 to 5 minutes. We\'ll look at your left eye first, then your right eye, and finally compare them. It\'s actually quite quick!')
    } else if (lowerCommand.includes('what happens') || lowerCommand.includes('explain') || lowerCommand.includes('process')) {
      speak('Here\'s exactly what we\'ll do: First, I\'ll help you position your face so the camera can see your eyes clearly. Then we\'ll capture images of each eye separately. Our AI will analyze these images for any signs of common vision conditions. I\'ll explain everything as we go.')
    } else if (lowerCommand.includes('accurate') || lowerCommand.includes('reliable') || lowerCommand.includes('trust')) {
      speak('Great question! Our AI has been trained on over 100,000 eye images and achieves 95% accuracy in detecting common conditions like myopia, glaucoma, and diabetic retinopathy. However, this is a screening tool - if we find anything concerning, I\'ll recommend you see an eye care professional.')
    } else if (lowerCommand.includes('pain') || lowerCommand.includes('hurt') || lowerCommand.includes('uncomfortable')) {
      speak('I\'m glad you asked! This test is completely painless. We only use your device\'s camera - no bright lights, no eye drops, no touching your eyes. It\'s as comfortable as video chatting with a friend.')
    } else if (lowerCommand.includes('privacy') || lowerCommand.includes('data') || lowerCommand.includes('secure')) {
      speak('Your privacy is absolutely protected. All image processing happens right here on your device when possible. We never share your health information without your explicit permission, and all data is encrypted.')
    } else if (lowerCommand.includes('ready') || lowerCommand.includes('start') || lowerCommand.includes('begin')) {
      speak('Wonderful! I can hear you\'re ready to begin. Let\'s start by making sure your face is positioned correctly in the camera. Look directly at your screen and I\'ll guide you through the alignment.')
    } else if (lowerCommand.includes('repeat') || lowerCommand.includes('again')) {
      if (currentQuestion) {
        speak(currentQuestion.question)
      } else {
        speak('I\'ll repeat that for you. Could you be more specific about what you\'d like me to repeat?')
      }
    } else if (lowerCommand.includes('help') || lowerCommand.includes('confused')) {
      speak('Of course! I\'m Dr. AI, and I\'m here to make this as easy as possible. You can ask me anything about the test, your eye health, or just chat with me if you\'re nervous. What would you like to know?')
    } else if (lowerCommand.includes('next') || lowerCommand.includes('continue')) {
      speak('Perfect! Let\'s move forward with your vision assessment.')
    } else if (lowerCommand.includes('thank you') || lowerCommand.includes('thanks')) {
      speak('You\'re so welcome! I\'m here to help. Taking care of your vision is important, and I\'m glad I can be part of that journey with you.')
    } else if (lowerCommand.includes('stop') || lowerCommand.includes('quiet') || lowerCommand.includes('pause')) {
      speak('Of course, I\'ll be quiet now. Just tap the microphone button if you need me again.')
      setIsActive(false)
      setIsQuestionMode(false)
      if (recognitionRef.current) {
        recognitionRef.current.stop()
      }
      setIsListening(false)
    } else {
      // More natural fallback
      speak('I want to make sure I understand you correctly. Could you rephrase that? I\'m here to help with anything about your vision test - just talk to me like you would talk to your doctor.')
    }
  }

  const toggleListening = () => {
    if (!recognitionRef.current) return
    
    if (isListening) {
      recognitionRef.current.stop()
      setIsListening(false)
    } else {
      recognitionRef.current.start()
      setIsListening(true)
      if (isQuestionMode && currentQuestion) {
        speak('I\'m listening for your answer.')
      } else {
        speak('I\'m listening. You can ask me to repeat instructions or say "help" for assistance.')
      }
    }
  }

  const toggleBot = () => {
    if (isActive) {
      setIsActive(false)
      if (recognitionRef.current) {
        recognitionRef.current.stop()
      }
      setIsListening(false)
      if ('speechSynthesis' in window) {
        window.speechSynthesis.cancel()
      }
    } else {
      setIsActive(true)
      provideWelcomeGuidance()
    }
  }

  // Expose functions globally for other components
  useEffect(() => {
    window.voiceBot = {
      speak,
      queueSpeech,
      provideGuidance: (text) => speak(text),
      startQuestions: startTestQuestions,
      analyzeScreen: (content) => screenAnalyzer.analyze(content)
    }

    return () => {
      delete window.voiceBot
    }
  }, [])

  // Load voices for fallback TTS
  useEffect(() => {
    if ('speechSynthesis' in window) {
      const loadVoices = () => {
        window.speechSynthesis.getVoices()
      }
      loadVoices()
      if (window.speechSynthesis.onvoiceschanged !== undefined) {
        window.speechSynthesis.onvoiceschanged = loadVoices
      }
    }
  }, [])

  if (!isActive) {
    return (
      <motion.button
        className="voice-bot-toggle"
        onClick={toggleBot}
        whileHover={{ scale: 1.1 }}
        whileTap={{ scale: 0.9 }}
        initial={{ scale: 0 }}
        animate={{ scale: 1 }}
        transition={{ type: 'spring', stiffness: 200 }}
      >
        <span className="mic-icon">ðŸŽ¤</span>
        <span className="toggle-label">Voice Guide</span>
      </motion.button>
    )
  }

  return (
    <motion.div
      className="voice-bot-container"
      initial={{ scale: 0, opacity: 0 }}
      animate={{ scale: 1, opacity: 1 }}
      exit={{ scale: 0, opacity: 0 }}
    >
      <div className="voice-bot-main">
        <div className="voice-bot-header">
          <div className="voice-bot-info">
            <div className="voice-bot-avatar">ðŸ©º</div>
            <div>
              <div className="voice-bot-name">Dr. AI</div>
              <div className="voice-bot-status">
                {isSpeaking ? 'Speaking...' : isListening ? 'Listening...' : 'Ready to help'}
              </div>
            </div>
          </div>
          <button
            className="voice-bot-close"
            onClick={toggleBot}
            title="Close Dr. AI"
          >
            Ã—
          </button>
        </div>

        <motion.div
          className={`mic-button ${isSpeaking ? 'speaking' : ''} ${isListening ? 'listening' : ''}`}
          onClick={toggleListening}
          animate={{
            scale: isSpeaking ? [1, 1.1, 1] : isListening ? [1, 1.05, 1] : 1,
          }}
          transition={{
            duration: 1.5,
            repeat: (isSpeaking || isListening) ? Infinity : 0,
            ease: 'easeInOut'
          }}
        >
          <span className="mic-icon-large">
            {isSpeaking ? 'ðŸ”Š' : isListening ? 'ðŸ‘‚' : 'ðŸŽ¤'}
          </span>
          {(isSpeaking || isListening) && (
            <motion.div
              className="sound-waves"
              initial={{ scale: 0 }}
              animate={{ scale: [1, 1.5, 1] }}
              transition={{ duration: 1, repeat: Infinity }}
            />
          )}
        </motion.div>
        
        <div className="voice-bot-instructions">
          <p>
            {isListening 
              ? "I'm listening... speak naturally!" 
              : isSpeaking 
              ? "I'm speaking to you..."
              : "Tap the microphone and ask me anything about your vision test!"
            }
          </p>
        </div>
      </div>
    </motion.div>
  )
}

export default VoiceBot


